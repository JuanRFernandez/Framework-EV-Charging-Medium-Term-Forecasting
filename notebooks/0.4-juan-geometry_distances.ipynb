{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point\n",
    "from typing import List\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from joblib import parallel_backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    I want to merge your df DataFrame, which contains information about charging stations, with the all_features_gdf GeoDataFrame that contains information about different kinds of amenities for different years. The goal is to enrich our charging station data with information about the surrounding amenities for each year.\n",
    "\n",
    "    Here is a strategy to do this:\n",
    "\n",
    "    1. Compute the distance between each charging station and all the amenities for a specific year.\n",
    "    2. Compute the descriptive statistics (total count, average distance, minimum distance, maximum distance, etc.) for each type of amenity.\n",
    "    3. Add these statistics as new columns to your charging stations DataFrame (df) for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gdfs():\n",
    "    # Get the current working directory\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    # Define the relative paths to the data files\n",
    "    all_features_gdf_path = os.path.join(cwd, '..', 'data', 'interim', 'all_features_gdf.csv')\n",
    "    df_path = os.path.join(cwd, '..', 'data', 'interim', 'train_gdf_forward_geocoded.csv')\n",
    "\n",
    "    # Load the dataframes\n",
    "    all_features_df = pd.read_csv(all_features_gdf_path)\n",
    "    df = pd.read_csv(df_path)\n",
    "\n",
    "    # Convert 'geometry' column to geometry type\n",
    "    all_features_df['geometry'] = all_features_df['geometry'].apply(wkt.loads)\n",
    "    df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "\n",
    "    # Convert the pandas DataFrames to GeoDataFrames\n",
    "    all_features_gdf = gpd.GeoDataFrame(all_features_df, geometry='geometry')\n",
    "    gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "    \n",
    "    return all_features_gdf, gdf\n",
    "\n",
    "all_features_gdf, gdf = load_gdfs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gdfs():\n",
    "    # Get the current working directory\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    # Define the relative paths to the data files\n",
    "    df_path = os.path.join(cwd, '..', 'data', 'interim', 'train_gdf_forward_geocoded.csv')\n",
    "\n",
    "    # Load the dataframes\n",
    "    df = pd.read_csv(df_path)\n",
    "\n",
    "    # Convert 'geometry' column to geometry type\n",
    "    df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "\n",
    "    # Convert the pandas DataFrames to GeoDataFrames\n",
    "    gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "gdf = load_gdfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>geometry</th>\n",
       "      <th>osm_year</th>\n",
       "      <th>fuel_station</th>\n",
       "      <th>parking_station</th>\n",
       "      <th>busstation_station</th>\n",
       "      <th>trainstation_station</th>\n",
       "      <th>mall_station</th>\n",
       "      <th>supermarket_station</th>\n",
       "      <th>restaurant_station</th>\n",
       "      <th>hotel_station</th>\n",
       "      <th>school_station</th>\n",
       "      <th>university_station</th>\n",
       "      <th>cinema_station</th>\n",
       "      <th>theatre_station</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>POINT (-4.34409 55.93436)</td>\n",
       "      <td>2016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>POINT (-3.72159 56.69738)</td>\n",
       "      <td>2016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>POINT (-3.16965 56.19748)</td>\n",
       "      <td>2016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>POINT (-3.29724 55.89943)</td>\n",
       "      <td>2016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>POINT (-3.28774 55.92326)</td>\n",
       "      <td>2016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                   geometry  osm_year  fuel_station  \\\n",
       "0           0  POINT (-4.34409 55.93436)      2016           1.0   \n",
       "1           1  POINT (-3.72159 56.69738)      2016           1.0   \n",
       "2           2  POINT (-3.16965 56.19748)      2016           1.0   \n",
       "3           3  POINT (-3.29724 55.89943)      2016           1.0   \n",
       "4           4  POINT (-3.28774 55.92326)      2016           1.0   \n",
       "\n",
       "   parking_station  busstation_station  trainstation_station  mall_station  \\\n",
       "0              NaN                 NaN                   NaN           NaN   \n",
       "1              NaN                 NaN                   NaN           NaN   \n",
       "2              NaN                 NaN                   NaN           NaN   \n",
       "3              NaN                 NaN                   NaN           NaN   \n",
       "4              NaN                 NaN                   NaN           NaN   \n",
       "\n",
       "   supermarket_station  restaurant_station  hotel_station  school_station  \\\n",
       "0                  NaN                 NaN            NaN             NaN   \n",
       "1                  NaN                 NaN            NaN             NaN   \n",
       "2                  NaN                 NaN            NaN             NaN   \n",
       "3                  NaN                 NaN            NaN             NaN   \n",
       "4                  NaN                 NaN            NaN             NaN   \n",
       "\n",
       "   university_station  cinema_station  theatre_station  \n",
       "0                 NaN             NaN              NaN  \n",
       "1                 NaN             NaN              NaN  \n",
       "2                 NaN             NaN              NaN  \n",
       "3                 NaN             NaN              NaN  \n",
       "4                 NaN             NaN              NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CP ID</th>\n",
       "      <th>Connector</th>\n",
       "      <th>Total kWh</th>\n",
       "      <th>Site</th>\n",
       "      <th>Model</th>\n",
       "      <th>End DateTime</th>\n",
       "      <th>Site_encoded</th>\n",
       "      <th>Model_encoded</th>\n",
       "      <th>Total kWh.1</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50994</td>\n",
       "      <td>1</td>\n",
       "      <td>2.084</td>\n",
       "      <td>Leslie Street Car Park</td>\n",
       "      <td>APT Triple Rapid Charger</td>\n",
       "      <td>2016-01-09 07:27:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.084</td>\n",
       "      <td>POINT (-3.33802 56.59132)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50281</td>\n",
       "      <td>2</td>\n",
       "      <td>3.870</td>\n",
       "      <td>Rie-Achan Road Car Park, Pitlochry</td>\n",
       "      <td>APT 22kW Dual Outlet</td>\n",
       "      <td>2016-01-09 09:01:00</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.870</td>\n",
       "      <td>POINT (-3.73882 56.70342)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>50285</td>\n",
       "      <td>1</td>\n",
       "      <td>13.930</td>\n",
       "      <td>Broxden Park &amp; Ride</td>\n",
       "      <td>APT 22kW Dual Outlet</td>\n",
       "      <td>2016-01-09 14:32:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.930</td>\n",
       "      <td>POINT (-3.47775 56.38661)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>50281</td>\n",
       "      <td>1</td>\n",
       "      <td>10.380</td>\n",
       "      <td>Rie-Achan Road Car Park, Pitlochry</td>\n",
       "      <td>APT 22kW Dual Outlet</td>\n",
       "      <td>2016-01-09 16:37:00</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.380</td>\n",
       "      <td>POINT (-3.73882 56.70342)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>50745</td>\n",
       "      <td>2</td>\n",
       "      <td>3.580</td>\n",
       "      <td>Kinross Park and Ride</td>\n",
       "      <td>APT Triple Rapid Charger</td>\n",
       "      <td>2016-01-09 09:37:00</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.580</td>\n",
       "      <td>POINT (-3.43295 56.20673)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  CP ID  Connector  Total kWh  \\\n",
       "0             0           0  50994          1      2.084   \n",
       "1             1           1  50281          2      3.870   \n",
       "2             2           2  50285          1     13.930   \n",
       "3             3           3  50281          1     10.380   \n",
       "4             4           4  50745          2      3.580   \n",
       "\n",
       "                                 Site                     Model  \\\n",
       "0              Leslie Street Car Park  APT Triple Rapid Charger   \n",
       "1  Rie-Achan Road Car Park, Pitlochry      APT 22kW Dual Outlet   \n",
       "2                 Broxden Park & Ride      APT 22kW Dual Outlet   \n",
       "3  Rie-Achan Road Car Park, Pitlochry      APT 22kW Dual Outlet   \n",
       "4               Kinross Park and Ride  APT Triple Rapid Charger   \n",
       "\n",
       "          End DateTime  Site_encoded  Model_encoded  Total kWh.1  \\\n",
       "0  2016-01-09 07:27:00           0.0            3.0        2.084   \n",
       "1  2016-01-09 09:01:00          19.0            0.0        3.870   \n",
       "2  2016-01-09 14:32:00           3.0            0.0       13.930   \n",
       "3  2016-01-09 16:37:00          19.0            0.0       10.380   \n",
       "4  2016-01-09 09:37:00          13.0            3.0        3.580   \n",
       "\n",
       "                    geometry  \n",
       "0  POINT (-3.33802 56.59132)  \n",
       "1  POINT (-3.73882 56.70342)  \n",
       "2  POINT (-3.47775 56.38661)  \n",
       "3  POINT (-3.73882 56.70342)  \n",
       "4  POINT (-3.43295 56.20673)  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define a function to compute distance between two geographical points (coordinates are in (latitude, longitude) format). Note that we'll use geopy library for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_amenity_distances(gdf: gpd.GeoDataFrame, amenity_gdf: gpd.GeoDataFrame, amenity: str, year: int, index: int) -> pd.DataFrame:\n",
    "    # Compute distances\n",
    "    station_coord = gdf.loc[index, 'geometry']\n",
    "    distances = amenity_gdf['geometry'].apply(\n",
    "        lambda x: geodesic((station_coord.y, station_coord.x), (x.y, x.x)).km\n",
    "    )\n",
    "\n",
    "    # Compute statistics\n",
    "    distances = distances.astype(float)\n",
    "    stats = {\n",
    "        f'{amenity}_count_{year}': len(distances),\n",
    "        f'{amenity}_avg_distance_{year}': distances.mean(),\n",
    "        f'{amenity}_min_distance_{year}': distances.min(),\n",
    "        f'{amenity}_max_distance_{year}': distances.max()\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(stats, index=[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, also we need to iterate over all rows in the df DataFrame and for each row (charging station), we compute the summary statistics for each amenity in the all_features_gdf GeoDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def calculate_row_amenity_stats(index: int, row: pd.Series, all_features_gdf: gpd.GeoDataFrame, amenities: list) -> pd.DataFrame:\n",
    "    all_stats = []\n",
    "    # Iterate over all years\n",
    "    for year in range(2016, 2020):  # adjust this as per your data\n",
    "        # print('year, :', year)\n",
    "        # Extract amenities for the year\n",
    "        year_amenities_gdf = all_features_gdf[all_features_gdf['osm_year'] == year]\n",
    "\n",
    "        # Iterate over all types of amenities\n",
    "        for amenity in amenities:\n",
    "            # Extract the GeoDataFrame for the specific amenity\n",
    "            amenity_gdf = year_amenities_gdf.dropna(subset=[amenity])\n",
    "\n",
    "            # Calculate distances and statistics for the amenity\n",
    "            stats = calculate_amenity_distances(row.to_frame().T, amenity_gdf, amenity, year, index)\n",
    "            all_stats.append(stats)\n",
    "\n",
    "    # Concatenate all stats dataframes\n",
    "    return pd.concat(all_stats, axis=0)\n",
    "\n",
    "# # Next, we need to create a function to compute the distances of a point to all amenities and return the summary statistics:\n",
    "# def calculate_amenity_stats(gdf: gpd.GeoDataFrame, all_features_gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "#     # Define all types of amenities\n",
    "#     amenities = ['fuel_station', 'parking_station', 'busstation_station', 'trainstation_station', \n",
    "#                     'mall_station', 'supermarket_station', 'restaurant_station', 'hotel_station', \n",
    "#                     'school_station', 'university_station', 'cinema_station', 'theatre_station']\n",
    "\n",
    "#     # Use joblib to parallelize the calculation with tqdm for progress update\n",
    "#     results = Parallel(n_jobs=-1)(delayed(calculate_row_amenity_stats)(index, row, all_features_gdf, amenities) for index, row in tqdm(gdf.iterrows(), total=gdf.shape[0]))\n",
    "\n",
    "#     # Join the results with the original geodataframe\n",
    "#     gdf = pd.concat([gdf, pd.concat(results, axis=0)], axis=1)\n",
    "\n",
    "#     return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def worker(args):\n",
    "#     index, row, all_features_gdf, amenities = args\n",
    "#     return calculate_row_amenity_stats(index, row, all_features_gdf, amenities)\n",
    "\n",
    "# def calculate_amenity_stats(gdf: gpd.GeoDataFrame, all_features_gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "#     amenities = ['fuel_station', 'parking_station', 'busstation_station', 'trainstation_station', \n",
    "#                     'mall_station', 'supermarket_station', 'restaurant_station', 'hotel_station', \n",
    "#                     'school_station', 'university_station', 'cinema_station', 'theatre_station']\n",
    "\n",
    "#     start_index = load_last_saved_index()\n",
    "#     checkpoint_interval = calculate_checkpoint_interval(gdf)\n",
    "\n",
    "#     results = []\n",
    "#     pool = Pool(processes=cpu_count())\n",
    "#     for index, result in tqdm(enumerate(pool.imap(worker, [(index, row, all_features_gdf, amenities) for index, row in gdf.iterrows()]), start=start_index), total=gdf.shape[0]):\n",
    "#         results.append(result)\n",
    "\n",
    "#         if is_checkpoint(index, checkpoint_interval):\n",
    "#             save_checkpoint(gdf, results, index)\n",
    "\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "\n",
    "#     return join_results(gdf, results)\n",
    "\n",
    "\n",
    "# def load_last_saved_index():\n",
    "#     cwd = os.getcwd()\n",
    "#     index_file_path = os.path.join(cwd, '..', 'data', 'interim', 'distance', 'last_saved_index.pkl')\n",
    "#     if os.path.exists(index_file_path):\n",
    "#         with open(index_file_path, 'rb') as f:\n",
    "#             return pickle.load(f)\n",
    "#     return 0\n",
    "\n",
    "\n",
    "# def calculate_checkpoint_interval(gdf):\n",
    "#     return gdf.shape[0] // 100\n",
    "\n",
    "\n",
    "# def is_checkpoint(index, interval):\n",
    "#     return (index + 1) % interval == 0\n",
    "\n",
    "\n",
    "# def save_checkpoint(gdf, results, index):\n",
    "#     cwd = os.getcwd()\n",
    "#     gdf_partial = pd.concat([gdf.loc[:index], pd.concat(results, axis=0)], axis=1)\n",
    "#     gdf_partial.to_pickle(os.path.join(cwd, '..', 'data', 'interim', 'distance', f'gdf_distance_save_{index + 1}.pkl'))\n",
    "#     save_last_saved_index(index)\n",
    "#     results.clear()\n",
    "\n",
    "\n",
    "# def save_last_saved_index(index):\n",
    "#     cwd = os.getcwd()\n",
    "#     with open(os.path.join(cwd, '..', 'data', 'interim', 'distance', 'last_saved_index.pkl'), 'wb') as f:\n",
    "#         pickle.dump(index, f)\n",
    "\n",
    "\n",
    "# def join_results(gdf, results):\n",
    "#     return pd.concat([gdf, pd.concat(results, axis=0)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# from joblib import parallel_backend\n",
    "\n",
    "# with parallel_backend('loky'):\n",
    "#     gdf = calculate_amenity_stats(gdf, all_features_gdf)\n",
    "\n",
    "# end_time = time.time()\n",
    "\n",
    "# print(f\"Total time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f94aa23815469ebd8617a0e15e7a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66664 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "InvalidIndexError",
     "evalue": "Reindexing only valid with uniquely valued Index objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 99\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39m# Use the 'loky' backend for joblib\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39m'\u001b[39m\u001b[39mloky\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     98\u001b[0m     \u001b[39m# Calculate the amenity stats for the GeoDataFrame\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     gdf \u001b[39m=\u001b[39m calculate_amenity_stats(gdf, all_features_gdf)\n\u001b[1;32m    101\u001b[0m \u001b[39m# Record the end time\u001b[39;00m\n\u001b[1;32m    102\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[10], line 34\u001b[0m, in \u001b[0;36mcalculate_amenity_stats\u001b[0;34m(gdf, all_features_gdf)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[39m# Save a checkpoint if necessary\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[39mif\u001b[39;00m is_checkpoint(index, checkpoint_interval):\n\u001b[0;32m---> 34\u001b[0m         save_checkpoint(gdf, results, index)\n\u001b[1;32m     36\u001b[0m \u001b[39m# Close the pool and wait for all processes to finish\u001b[39;00m\n\u001b[1;32m     37\u001b[0m pool\u001b[39m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[10], line 68\u001b[0m, in \u001b[0;36msave_checkpoint\u001b[0;34m(gdf, results, index)\u001b[0m\n\u001b[1;32m     66\u001b[0m cwd \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mgetcwd()\n\u001b[1;32m     67\u001b[0m \u001b[39m# Combine the current state of the GeoDataFrame with the results so far\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m gdf_partial \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat([gdf\u001b[39m.\u001b[39;49mloc[:index], pd\u001b[39m.\u001b[39;49mconcat(results, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)], axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     69\u001b[0m \u001b[39m# Save the combined data to a pickle file\u001b[39;00m\n\u001b[1;32m     70\u001b[0m gdf_partial\u001b[39m.\u001b[39mto_pickle(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(cwd, \u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minterim\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdistances\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgdf_distance_save_\u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.pkl\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/envs/ev_charging_env/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/ev_charging_env/lib/python3.10/site-packages/pandas/core/reshape/concat.py:381\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39mConcatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39m1   3   4\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    368\u001b[0m op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[1;32m    369\u001b[0m     objs,\n\u001b[1;32m    370\u001b[0m     axis\u001b[39m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    378\u001b[0m     sort\u001b[39m=\u001b[39msort,\n\u001b[1;32m    379\u001b[0m )\n\u001b[0;32m--> 381\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mget_result()\n",
      "File \u001b[0;32m/opt/conda/envs/ev_charging_env/lib/python3.10/site-packages/pandas/core/reshape/concat.py:612\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    610\u001b[0m         obj_labels \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39maxes[\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m ax]\n\u001b[1;32m    611\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m new_labels\u001b[39m.\u001b[39mequals(obj_labels):\n\u001b[0;32m--> 612\u001b[0m             indexers[ax] \u001b[39m=\u001b[39m obj_labels\u001b[39m.\u001b[39;49mget_indexer(new_labels)\n\u001b[1;32m    614\u001b[0m     mgrs_indexers\u001b[39m.\u001b[39mappend((obj\u001b[39m.\u001b[39m_mgr, indexers))\n\u001b[1;32m    616\u001b[0m new_data \u001b[39m=\u001b[39m concatenate_managers(\n\u001b[1;32m    617\u001b[0m     mgrs_indexers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_axes, concat_axis\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbm_axis, copy\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy\n\u001b[1;32m    618\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/ev_charging_env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3904\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3901\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_method(method, limit, tolerance)\n\u001b[1;32m   3903\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 3904\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidIndexError(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_requires_unique_msg)\n\u001b[1;32m   3906\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(target) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   3907\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray([], dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mintp)\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: Reindexing only valid with uniquely valued Index objects"
     ]
    }
   ],
   "source": [
    "# This function calculates the amenity stats for a given row\n",
    "# It is called in parallel by the multiprocessing pool\n",
    "def worker(args):\n",
    "    index, row, all_features_gdf, amenities = args\n",
    "    result = calculate_row_amenity_stats(index, row, all_features_gdf, amenities)\n",
    "    # Reset the index of the result before returning it\n",
    "    result = result.reset_index(drop=True)\n",
    "    return result\n",
    "\n",
    "# This function calculates amenity stats for the entire GeoDataFrame\n",
    "# It uses multiprocessing to speed up the process\n",
    "def calculate_amenity_stats(gdf: gpd.GeoDataFrame, all_features_gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    # Define the amenities we are interested in\n",
    "    amenities = ['fuel_station', 'parking_station', 'busstation_station', 'trainstation_station', \n",
    "                    'mall_station', 'supermarket_station', 'restaurant_station', 'hotel_station', \n",
    "                    'school_station', 'university_station', 'cinema_station', 'theatre_station']\n",
    "\n",
    "    # Load the last saved index to resume work in case of interruption\n",
    "    start_index = load_last_saved_index()\n",
    "    # Calculate the checkpoint interval for saving progress\n",
    "    checkpoint_interval = calculate_checkpoint_interval(gdf)\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    results = []\n",
    "    # Create a multiprocessing pool\n",
    "    pool = Pool(processes=cpu_count())\n",
    "    # Use the pool to process the data in parallel\n",
    "    for index, result in tqdm(enumerate(pool.imap(worker, [(index, row, all_features_gdf, amenities) for index, row in gdf.iterrows()]), start=start_index), total=gdf.shape[0]):\n",
    "        # Append the result to the results list\n",
    "        results.append(result)\n",
    "\n",
    "        # Save a checkpoint if necessary\n",
    "        if is_checkpoint(index, checkpoint_interval):\n",
    "            save_checkpoint(gdf, results, index)\n",
    "\n",
    "    # Close the pool and wait for all processes to finish\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Combine the results with the original GeoDataFrame\n",
    "    return join_results(gdf, results)\n",
    "\n",
    "# This function loads the last saved index from a pickle file\n",
    "def load_last_saved_index():\n",
    "    # Define the path to the pickle file\n",
    "    cwd = os.getcwd()\n",
    "    index_file_path = os.path.join(cwd, '..', 'data', 'interim', 'distance', 'last_saved_index.pkl')\n",
    "    # If the file exists, load the index from it\n",
    "    if os.path.exists(index_file_path):\n",
    "        with open(index_file_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    # If the file does not exist, return 0\n",
    "    return 0\n",
    "\n",
    "# This function calculates the interval at which checkpoints should be saved\n",
    "# It is set to 1% of the total number of rows in the GeoDataFrame\n",
    "def calculate_checkpoint_interval(gdf):\n",
    "    return gdf.shape[0] // 100\n",
    "\n",
    "# This function checks if a checkpoint should be saved at the current index\n",
    "def is_checkpoint(index, interval):\n",
    "    return (index + 1) % interval == 0\n",
    "\n",
    "# This function saves a checkpoint by saving the current state of the GeoDataFrame and the results to a pickle file\n",
    "def save_checkpoint(gdf, results, index):\n",
    "    cwd = os.getcwd()\n",
    "    # Combine the current state of the GeoDataFrame with the results so far\n",
    "    gdf_partial = pd.concat([gdf.loc[:index], pd.concat(results, axis=0)], axis=1)\n",
    "    # Save the combined data to a pickle file\n",
    "    gdf_partial.to_pickle(os.path.join(cwd, '..', 'data', 'interim', 'distances', f'gdf_distance_save_{index + 1}.pkl'))\n",
    "    # Save the current index to a separate pickle file\n",
    "    save_last_saved_index(index)\n",
    "    # Clear the results list to free up memory\n",
    "    results.clear()\n",
    "\n",
    "# This function saves the last saved index to a pickle file\n",
    "def save_last_saved_index(index):\n",
    "    cwd = os.getcwd()\n",
    "    with open(os.path.join(cwd, '..', 'data', 'interim', 'distances', 'last_saved_index.pkl'), 'wb') as f:\n",
    "        pickle.dump(index, f)\n",
    "\n",
    "# This function combines the results with the original GeoDataFrame\n",
    "def join_results(gdf, results):\n",
    "    return pd.concat([gdf, pd.concat(results, axis=0)], axis=1)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "\n",
    "    # Record the start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Reset the index of the GeoDataFrame\n",
    "    gdf = gdf.reset_index(drop=True)\n",
    "\n",
    "    # Use the 'loky' backend for joblib\n",
    "    with parallel_backend('loky'):\n",
    "        # Calculate the amenity stats for the GeoDataFrame\n",
    "        gdf = calculate_amenity_stats(gdf, all_features_gdf)\n",
    "\n",
    "    # Record the end time\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Print the total execution time\n",
    "    print(f\"Total time: {end_time - start_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ev_charging_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
